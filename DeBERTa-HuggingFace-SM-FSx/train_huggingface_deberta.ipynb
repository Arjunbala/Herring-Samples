{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2fe64f2",
   "metadata": {},
   "source": [
    "## Distributed Data Parallel DeBERTa Training with HuggingFace Transformers on SageMaker\n",
    "\n",
    "Amazon SageMaker's distributed library can be used to train deep learning models faster and cheaper. The data parallel feature in this library (smdistributed.dataparallel) is a distributed data parallel training framework that can work with a variety of frameworks including PyTorch and TensorFlow, as well as higher level toolkits such as HuggingFace transformers.\n",
    "\n",
    "In July 2021, AWS and HuggingFace collaborated to introduce [HuggingFace Deep Learning Containers (DLCs)](https://huggingface.co/transformers/v4.8.2/sagemaker.html#deep-learning-container-dlc-overview) which have fully integrated support with SageMaker to make it easier to develop and ship cutting-edge NLP models.\n",
    "\n",
    "In this notebook, we shall see how to leverage HuggingFace DLCs and the [SageMaker HuggingFace estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) to pre-train [DeBERTa model](https://arxiv.org/abs/2006.03654) on [SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/) for a question-answering task.\n",
    "\n",
    "The outline of steps is as follows:\n",
    "\n",
    "- Install pre-requisites required for using SageMaker and HuggingFace transformers.\n",
    "- Pre-process the SQuAD dataset and stage it in Amazon S3.\n",
    "- Use HuggingFace Estimator to pre-train DeBERTa model on SQuAD dataset.\n",
    "\n",
    "NOTE: This example requires SageMaker Python SDK v2.X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb497c",
   "metadata": {},
   "source": [
    "### Prepare SageMaker Environment\n",
    "\n",
    "To get started, we need to set up the environment with a few pre-requisite steps.\n",
    "\n",
    "#### Pre-requisite Installations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b512c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (2.69.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.80.0.tar.gz (517 kB)\n",
      "     |████████████████████████████████| 517 kB 9.2 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: botocore in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (1.24.21)\n",
      "Collecting botocore\n",
      "  Downloading botocore-1.24.24-py3-none-any.whl (8.6 MB)\n",
      "     |████████████████████████████████| 8.6 MB 95.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (1.17.49)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.21.24-py3-none-any.whl (132 kB)\n",
      "     |████████████████████████████████| 132 kB 122.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: awscli in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (1.22.5)\n",
      "Collecting awscli\n",
      "  Downloading awscli-1.22.79-py3-none-any.whl (3.8 MB)\n",
      "     |████████████████████████████████| 3.8 MB 73.5 MB/s            \n",
      "\u001b[?25hCollecting attrs==20.3.0\n",
      "  Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (1.21.4)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (3.19.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (4.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (21.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (1.3.4)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from botocore) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from botocore) (1.26.7)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from boto3) (0.5.2)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from awscli) (4.7.2)\n",
      "Collecting PyYAML<5.5,>=3.10\n",
      "  Using cached PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Collecting colorama<0.4.4,>=0.2.5\n",
      "  Using cached colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.80.0-py2.py3-none-any.whl size=716399 sha256=801a92058206ceeac4a43add97e78744eb19e93ac9bc8289c1886b3db384bbdc\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/b9/1f/23/a7c818b46ba5d43c94faf5e19b177962152f4d939a91ab220d\n",
      "Successfully built sagemaker\n",
      "\u001b[33mWARNING: Error parsing requirements for pyyaml: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/PyYAML-6.0.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: botocore, PyYAML, docutils, colorama, boto3, attrs, sagemaker, awscli\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.24.21\n",
      "    Uninstalling botocore-1.24.21:\n",
      "      Successfully uninstalled botocore-1.24.21\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "\u001b[31mERROR: Cannot uninstall PyYAML 6.0, RECORD file not found. You might be able to recover from this via: 'pip install --force-reinstall --no-deps PyYAML==6.0'.\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (4.6.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (1.6.2)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.0.0-py3-none-any.whl (325 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (3.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (4.49.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers!=0.11.3,>=0.11.1\n",
      "  Using cached tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (21.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (2021.11.2)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (6.0.0)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (2021.11.0)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (1.3.4)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/PyYAML-6.0.dist-info/METADATA'\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker botocore boto3 awscli --upgrade\n",
    "!pip install transformers datasets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bec8d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker: 2.68.0\n",
      "transformers: 4.6.1\n",
      "datasets: 1.6.2\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "import sagemaker\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(f\"sagemaker: {sagemaker.__version__}\")\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"datasets: {datasets.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21303bec",
   "metadata": {},
   "source": [
    "Copy and run the following code if you need to upgrade ipywidgets for datasets library and restart kernel. This is only needed when prerpocessing is done in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5eca47ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "# has to restart kernel for the updates to be applied\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b727d",
   "metadata": {},
   "source": [
    "#### SageMaker Environment\n",
    "\n",
    "Note: If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for SageMaker. To learn more, see [SageMaker Roles](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0e92c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::570106654206:role/Dev\n",
      "sagemaker bucket: sagemaker-us-west-2-570106654206\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it does not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95435f82",
   "metadata": {},
   "source": [
    "#### Preparing the SQuAD dataset\n",
    "\n",
    "When using the 🤗 [Datasets library](https://github.com/huggingface/datasets), datasets can be downloaded directly with the following datasets.load_dataset() method:\n",
    "\n",
    "from datasets import load_dataset\n",
    "load_dataset('dataset_name')\n",
    "\n",
    "If you'd like to try other training datasets later, you can simply use this method.\n",
    "\n",
    "For this example notebook, we prepared the SQuAD v1.1 dataset in the public SageMaker sample file S3 bucket. The following code cells show how you can directly load the dataset and convert to a HuggingFace DatasetDict.\n",
    "\n",
    "NOTE: The [SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/) is under the [CC BY-SA 4.0 license terms](https://creativecommons.org/licenses/by-sa/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9916e64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.4'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from datasets.filesystems import S3FileSystem\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b8fcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'context', 'question', 'id', 'answers'],\n",
       "        num_rows: 8672301\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'context', 'question', 'id', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper function to grab the dataset and load into DatasetDict\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "\n",
    "def make_split(split):\n",
    "    if split == \"train\":\n",
    "        file = \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/text/squad/train-v1.1.json\"\n",
    "    elif split == \"test\":\n",
    "        file = \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/text/squad/dev-v1.1.json\"\n",
    "    with urllib.request.urlopen(file) as f:\n",
    "        squad = json.load(f)\n",
    "        data = []\n",
    "        for article in squad[\"data\"]:\n",
    "            title = article.get(\"title\", \"\")\n",
    "            for paragraph in article[\"paragraphs\"]:\n",
    "                context = paragraph[\"context\"]  # do not strip leading blank spaces GH-2585\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n",
    "                    answers = [answer[\"text\"] for answer in qa[\"answers\"]]\n",
    "                    # Features currently used are \"context\", \"question\", and \"answers\".\n",
    "                    # Others are extracted here for the ease of future expansions.\n",
    "                    if split == \"train\":\n",
    "                        for i in range (1,100):\n",
    "                            data.append(\n",
    "                            {\n",
    "                                \"title\": title,\n",
    "                                \"context\": context,\n",
    "                                \"question\": qa[\"question\"],\n",
    "                                \"id\": qa[\"id\"],\n",
    "                                \"answers\": {\n",
    "                                    \"answer_start\": answer_starts,\n",
    "                                    \"text\": answers,\n",
    "                                },\n",
    "                            }\n",
    "                            )\n",
    "                    else:\n",
    "                        data.append(\n",
    "                            {\n",
    "                                \"title\": title,\n",
    "                                \"context\": context,\n",
    "                                \"question\": qa[\"question\"],\n",
    "                                \"id\": qa[\"id\"],\n",
    "                                \"answers\": {\n",
    "                                    \"answer_start\": answer_starts,\n",
    "                                    \"text\": answers,\n",
    "                                },\n",
    "                            }\n",
    "                            )\n",
    "        df = pd.DataFrame(data)\n",
    "        return Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "train = make_split(\"train\")\n",
    "test = make_split(\"test\")\n",
    "\n",
    "datasets = DatasetDict()\n",
    "datasets[\"train\"] = train\n",
    "datasets[\"validation\"] = test\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ebba1",
   "metadata": {},
   "source": [
    "#### Pre-processing the SQuAD dataset\n",
    "\n",
    "Before we can feed those texts to the Trainer model, we need to preprocess them. This can be done by a 🤗 Transformers Tokenizer which (as the name indicates) tokenizes the input texts (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put them into a format the model expects, as well as generate other inputs that the model requires.\n",
    "\n",
    "To do this, we instantiate a tokenizer using the AutoTokenizer.from_pretrained method, which will ensure that:\n",
    "\n",
    "- We get a tokenizer that corresponds to the model architecture we want to use.\n",
    "- We download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again when you run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50e8c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"microsoft/deberta-base\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc617409",
   "metadata": {},
   "source": [
    "The following assertion ensures that our tokenizer is a fast tokenizers (backed by Rust) from the 🤗 Tokenizers library. Those fast tokenizers are available for almost all models, and we will need some of the special features they have for our preprocessing. You can check which type of models have a fast tokenizer available and which don't on the [big table of models](https://huggingface.co/docs/transformers/index#bigtable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecadfc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384  # The maximum length of a feature (question and context)\n",
    "doc_stride = (\n",
    "    128  # The authorized overlap between two parts of the context when splitting it is needed.\n",
    ")\n",
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529a087",
   "metadata": {},
   "source": [
    "Now, let's put everything together in one function that we will apply to our training set. In the case of impossible answers (the answer is in another feature given by an example with a long context), we set the `cls` index for both the start and end position. We could also simply discard those examples from the training set if the flag `allow_impossible_answers` is `False`. Because the preprocessing is already complex enough as it is, we've kept is simple for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93d72e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possibly giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (\n",
    "                offsets[token_start_index][0] <= start_char\n",
    "                and offsets[token_end_index][1] >= end_char\n",
    "            ):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while (\n",
    "                    token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char\n",
    "                ):\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a90d9e",
   "metadata": {},
   "source": [
    "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation, and testing data will be preprocessed in one single command. Since our preprocessing changes the number of samples, we need to remove the old columns when applying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3c6dab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986ae292b7364569accd9ebf07a06418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8673.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cceae83f6c4f21a6e117df1b172763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "    prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ec5e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "train_dataset.set_format(\n",
    "    \"torch\", columns=[\"attention_mask\", \"end_positions\", \"input_ids\", \"start_positions\"]\n",
    ")\n",
    "eval_dataset.set_format(\n",
    "    \"torch\", columns=[\"attention_mask\", \"end_positions\", \"input_ids\", \"start_positions\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92acb5d0",
   "metadata": {},
   "source": [
    "Before we kick off our SageMaker training job we need to transfer our dataset to S3 so the training job can download it from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d487eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-570106654206/samples/datasets/squad-8million/train\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "s3_prefix = \"samples/datasets/squad-8million\"\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/train\"\n",
    "print(training_input_path)\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "eval_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/eval\"\n",
    "eval_dataset.save_to_disk(eval_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f70de9a",
   "metadata": {},
   "source": [
    "### SageMaker Training Job\n",
    "\n",
    "To create a SageMaker training job, we use a `HuggingFace Estimator`. Using the estimator, you can define which script should SageMaker use through `entry_point`, which `instance_type` to use for training, which `hyperparameters` to pass, and so on.\n",
    "\n",
    "When a SageMaker training job starts, SageMaker takes care of starting and managing all the required machine learning instances, picks up the `HuggingFace Deep Learning Container`, uploads your training script, and downloads the data from `sagemaker_session_bucket` into the container at `/opt/ml/input/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "627437a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "subnets = [\"subnet-031c4b8933ae6f4fb\"]  # Should be same as Subnet used for FSx. Example: subnet-0f9XXXX\n",
    "security_group_ids = [\"sg-095b065a4a6bb7136\"]  # Should be same as Security group used for FSx. sg-03ZZZZZZ\n",
    "job_name = \"smdataparallel-deberta-fsx-trial-16\"  # This job name is used as prefix to the sagemaker training job. Makes it easy for your look for your training job in SageMaker Training job console.\n",
    "file_system_id = \"fs-01cd8aadfdbf45e8c\"  # FSx file system ID with your training dataset. Example: 'fs-0bYYYYYY'\n",
    "\n",
    "# hyperparameters, which are passed into the training script\n",
    "hyperparameters={\n",
    "    'epochs': 20,                                    \n",
    "    'train_batch_size': 16,                         \n",
    "    'eval_batch_size': 16,                          \n",
    "    'learning_rate': 3e-5*8\n",
    "}\n",
    "\n",
    "# refer https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers to get the right uri's based on region\n",
    "#image_uri = '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.9.1-transformers4.12.3-gpu-py38-cu111-ubuntu20.04'\n",
    "image_uri = '570106654206.dkr.ecr.us-west-2.amazonaws.com/pt-hf-110\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "# this is the only line of code change required to leverage SageMaker Distributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'qa_deberta_huggingface_trainer.py',       \n",
    "    source_dir           = './deberta_script',       \n",
    "    instance_type        = 'ml.p4d.24xlarge',   \n",
    "    instance_count       = 1, \n",
    "    role                 = role,             \n",
    "    py_version           = 'py38',  \n",
    "    sagemaker_session    = sess,\n",
    "    image_uri            = image_uri,\n",
    "    hyperparameters      = hyperparameters,   \n",
    "    subnets              = subnets,\n",
    "    security_group_ids   = security_group_ids,\n",
    "    debugger_hook_config = False,\n",
    "    distribution         = distribution,\n",
    "    max_retry_attempts   = 30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48734dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FSx Input for your SageMaker Training job\n",
    "\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "# YOUR_MOUNT_PATH_FOR_TRAINING_DATA # Example: '/fsx/squad...''''\n",
    "file_system_directory_path_train = \"/nx36zbmv/squad/squad/train\"\n",
    "file_system_directory_path_eval = \"/nx36zbmv/squad/squad/eval\"\n",
    "#file_system_directory_path_train = \"/uzr6zbmv/squad/squad/train\"\n",
    "#file_system_directory_path_eval = \"/uzr6zbmv/squad/squad/eval\"\n",
    "file_system_access_mode = \"rw\"\n",
    "file_system_type = \"FSxLustre\"\n",
    "train_fs = FileSystemInput(\n",
    "    file_system_id=file_system_id,\n",
    "    file_system_type=file_system_type,\n",
    "    directory_path=file_system_directory_path_train,\n",
    "    file_system_access_mode=file_system_access_mode,\n",
    ")\n",
    "eval_fs = FileSystemInput(\n",
    "    file_system_id=file_system_id,\n",
    "    file_system_type=file_system_type,\n",
    "    directory_path=file_system_directory_path_eval,\n",
    "    file_system_access_mode=file_system_access_mode,\n",
    ")\n",
    "data_channels = {\"train\": train_fs, \"eval\": eval_fs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d0db6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with data in FSx as input\n",
    "huggingface_estimator.fit(inputs=data_channels,job_name=job_name,wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a2e1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p37",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
