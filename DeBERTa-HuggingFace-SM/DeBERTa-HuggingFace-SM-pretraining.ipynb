{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de9e606",
   "metadata": {},
   "source": [
    "## Distributed Data Parallel DeBERTa Training with HuggingFace Transformers on SageMaker\n",
    "\n",
    "Amazon SageMaker's distributed library can be used to train deep learning models faster and cheaper. The data parallel feature in this library (smdistributed.dataparallel) is a distributed data parallel training framework that can work with a variety of frameworks including PyTorch and TensorFlow, as well as higher level toolkits such as HuggingFace transformers.\n",
    "\n",
    "In July 2021, AWS and HuggingFace collaborated to introduce [HuggingFace Deep Learning Containers (DLCs)](https://huggingface.co/transformers/v4.8.2/sagemaker.html#deep-learning-container-dlc-overview) which have fully integrated support with SageMaker to make it easier to develop and ship cutting-edge NLP models.\n",
    "\n",
    "In this notebook, we shall see how to leverage HuggingFace DLCs and the [SageMaker HuggingFace estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) to pre-train [DeBERTa model](https://arxiv.org/abs/2006.03654) on [SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/) for a question-answering task.\n",
    "\n",
    "The outline of steps is as follows:\n",
    "\n",
    "- Install pre-requisites required for using SageMaker and HuggingFace transformers.\n",
    "- Pre-process the SQuAD dataset and stage it in Amazon S3.\n",
    "- Use HuggingFace Estimator to pre-train DeBERTa model on SQuAD dataset.\n",
    "\n",
    "NOTE: This example requires SageMaker Python SDK v2.X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b297cf76",
   "metadata": {},
   "source": [
    "### Prepare SageMaker Environment\n",
    "\n",
    "To get started, we need to set up the environment with a few pre-requisite steps.\n",
    "\n",
    "#### Pre-requisite Installations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcec68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker botocore boto3 awscli --upgrade\n",
    "!pip install transformers datasets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59943b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "import sagemaker\n",
    "import transformers\n",
    "\n",
    "print(f\"sagemaker: {sagemaker.__version__}\")\n",
    "print(f\"transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84c9be",
   "metadata": {},
   "source": [
    "Copy and run the following code if you need to upgrade ipywidgets for datasets library and restart kernel. This is only needed when prerpocessing is done in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f053b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "# has to restart kernel for the updates to be applied\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf8222",
   "metadata": {},
   "source": [
    "#### SageMaker Environment\n",
    "\n",
    "Note: If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for SageMaker. To learn more, see [SageMaker Roles](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it does not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377e01e5",
   "metadata": {},
   "source": [
    "#### Preparing the SQuAD dataset\n",
    "\n",
    "When using the ðŸ¤— [Datasets library](https://github.com/huggingface/datasets), datasets can be downloaded directly with the following datasets.load_dataset() method:\n",
    "\n",
    "from datasets import load_dataset\n",
    "load_dataset('dataset_name')\n",
    "\n",
    "If you'd like to try other training datasets later, you can simply use this method.\n",
    "\n",
    "For this example notebook, we prepared the SQuAD v1.1 dataset in the public SageMaker sample file S3 bucket. The following code cells show how you can directly load the dataset and convert to a HuggingFace DatasetDict.\n",
    "\n",
    "NOTE: The [SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/) is under the [CC BY-SA 4.0 license terms](https://creativecommons.org/licenses/by-sa/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbdca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from datasets.filesystems import S3FileSystem\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to grab the dataset and load into DatasetDict\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "\n",
    "def make_split(split):\n",
    "    if split == \"train\":\n",
    "        file = \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/text/squad/train-v1.1.json\"\n",
    "    elif split == \"test\":\n",
    "        file = \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/text/squad/dev-v1.1.json\"\n",
    "    with urllib.request.urlopen(file) as f:\n",
    "        squad = json.load(f)\n",
    "        data = []\n",
    "        for article in squad[\"data\"]:\n",
    "            title = article.get(\"title\", \"\")\n",
    "            for paragraph in article[\"paragraphs\"]:\n",
    "                context = paragraph[\"context\"]  # do not strip leading blank spaces GH-2585\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n",
    "                    answers = [answer[\"text\"] for answer in qa[\"answers\"]]\n",
    "                    # Features currently used are \"context\", \"question\", and \"answers\".\n",
    "                    # Others are extracted here for the ease of future expansions.\n",
    "                    data.append(\n",
    "                        {\n",
    "                            \"title\": title,\n",
    "                            \"context\": context,\n",
    "                            \"question\": qa[\"question\"],\n",
    "                            \"id\": qa[\"id\"],\n",
    "                            \"answers\": {\n",
    "                                \"answer_start\": answer_starts,\n",
    "                                \"text\": answers,\n",
    "                            },\n",
    "                        }\n",
    "                    )\n",
    "        df = pd.DataFrame(data)\n",
    "        return Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "train = make_split(\"train\")\n",
    "test = make_split(\"test\")\n",
    "\n",
    "datasets = DatasetDict()\n",
    "datasets[\"train\"] = train\n",
    "datasets[\"validation\"] = test\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f1d27",
   "metadata": {},
   "source": [
    "#### Pre-processing the SQuAD dataset\n",
    "\n",
    "Before we can feed those texts to the Trainer model, we need to preprocess them. This can be done by a ðŸ¤— Transformers Tokenizer which (as the name indicates) tokenizes the input texts (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put them into a format the model expects, as well as generate other inputs that the model requires.\n",
    "\n",
    "To do this, we instantiate a tokenizer using the AutoTokenizer.from_pretrained method, which will ensure that:\n",
    "\n",
    "- We get a tokenizer that corresponds to the model architecture we want to use.\n",
    "- We download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again when you run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"microsoft/deberta-base\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48741444",
   "metadata": {},
   "source": [
    "The following assertion ensures that our tokenizer is a fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library. Those fast tokenizers are available for almost all models, and we will need some of the special features they have for our preprocessing. You can check which type of models have a fast tokenizer available and which don't on the [big table of models](https://huggingface.co/docs/transformers/index#bigtable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384  # The maximum length of a feature (question and context)\n",
    "doc_stride = (\n",
    "    128  # The authorized overlap between two parts of the context when splitting it is needed.\n",
    ")\n",
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d8fc7",
   "metadata": {},
   "source": [
    "Now, let's put everything together in one function that we will apply to our training set. In the case of impossible answers (the answer is in another feature given by an example with a long context), we set the `cls` index for both the start and end position. We could also simply discard those examples from the training set if the flag `allow_impossible_answers` is `False`. Because the preprocessing is already complex enough as it is, we've kept is simple for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faaae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possibly giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (\n",
    "                offsets[token_start_index][0] <= start_char\n",
    "                and offsets[token_end_index][1] >= end_char\n",
    "            ):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while (\n",
    "                    token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char\n",
    "                ):\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8588a9",
   "metadata": {},
   "source": [
    "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation, and testing data will be preprocessed in one single command. Since our preprocessing changes the number of samples, we need to remove the old columns when applying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "    prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c9125",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "train_dataset.set_format(\n",
    "    \"torch\", columns=[\"attention_mask\", \"end_positions\", \"input_ids\", \"start_positions\"]\n",
    ")\n",
    "eval_dataset.set_format(\n",
    "    \"torch\", columns=[\"attention_mask\", \"end_positions\", \"input_ids\", \"start_positions\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e5beb",
   "metadata": {},
   "source": [
    "Before we kick off our SageMaker training job we need to transfer our dataset to S3 so the training job can download it from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91043251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "s3_prefix = \"samples/datasets/squad\"\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/train\"\n",
    "print(training_input_path)\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "eval_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/eval\"\n",
    "eval_dataset.save_to_disk(eval_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc708e",
   "metadata": {},
   "source": [
    "### SageMaker Training Job\n",
    "\n",
    "To create a SageMaker training job, we use a `HuggingFace Estimator`. Using the estimator, you can define which script should SageMaker use through `entry_point`, which `instance_type` to use for training, which `hyperparameters` to pass, and so on.\n",
    "\n",
    "When a SageMaker training job starts, SageMaker takes care of starting and managing all the required machine learning instances, picks up the `HuggingFace Deep Learning Container`, uploads your training script, and downloads the data from `sagemaker_session_bucket` into the container at `/opt/ml/input/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36883712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training script\n",
    "hyperparameters={\n",
    "    'epochs': 20,                                    \n",
    "    'train_batch_size': 16,                         \n",
    "    'eval_batch_size': 16,                          \n",
    "    'learning_rate': 3e-5*8\n",
    "}\n",
    "\n",
    "# refer https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers to get the right uri's based on region\n",
    "image_uri = '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.9.1-transformers4.12.3-gpu-py38-cu111-ubuntu20.04'\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "# this is the only line of code change required to leverage SageMaker Distributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'qa_deberta_huggingface_trainer.py',       \n",
    "    source_dir           = './deberta_script',       \n",
    "    instance_type        = 'ml.p4d.24xlarge',   \n",
    "    instance_count       = 2, \n",
    "    role                 = role,             \n",
    "    py_version           = 'py38',            \n",
    "    image_uri            = image_uri,\n",
    "    hyperparameters      = hyperparameters,   \n",
    "    distribution         = distribution,\n",
    "    max_retry_attempts   = 30\n",
    ")\n",
    "\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "    'train': training_input_path,\n",
    "    'eval': eval_input_path\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data,wait=False)\n",
    "\n",
    "# The name of the training job. You might need to note this down in case you lose connection to your notebook.\n",
    "print(huggingface_estimator.latest_training_job.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p37",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
